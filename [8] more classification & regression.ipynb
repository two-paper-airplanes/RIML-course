{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Classification using k-Nearest Neighbors\n","\n","We'll give k-NN a try using the well known [wine dataset](https://archive.ics.uci.edu/dataset/109/wine). These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n"],"metadata":{"id":"s9w5Y24G_cCv"}},{"cell_type":"code","source":["# Import the necessary libraries\n","from sklearn.datasets import load_wine\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from sklearn.inspection import DecisionBoundaryDisplay\n","from matplotlib.colors import ListedColormap\n","import seaborn as sns"],"metadata":{"id":"NPKng2tPjU1k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the wine dataset\n","wine = load_wine()"],"metadata":{"id":"JzmTwcQjjVfq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wine.feature_names"],"metadata":{"id":"ZbBuEd6zjVyW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll split the dataset into training and testing arrays. For today, we'll choose two features to train the model with. In this first example, we choose the first two features: alcohol and malic acid.\n","\n","One data is loaded, we can easily use the `KNeighborsClassifier` function. One downside is that We need to specify how many classes. In this case we already know that there are 3 classes (i.e. 3 different cultivars), but usually we would need to take the extra step of testing with different class numbers and seeing which fit the data the best."],"metadata":{"id":"e6oaoueotj-5"}},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(wine.data[:, :2], wine.target, test_size=0.2, random_state=0)\n","\n","# Create a KNN classifier with 3 nearest neighbours\n","knn = KNeighborsClassifier(n_neighbors=3)\n","\n","# Train the classifier on the training data\n","knn.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = knn.predict(X_test)\n","\n","# Evaluate the accuracy of the classifier\n","accuracy = knn.score(X_test, y_test)\n","print(f\"Accuracy: {accuracy:.3f}\")"],"metadata":{"id":"r4OtCRVwAC-U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can plot out a visualisation of the classification results too:"],"metadata":{"id":"jOjnHU8muWA3"}},{"cell_type":"code","source":["# Plot out decision boundary\n","cmap_light = ListedColormap([\"cornflowerblue\",\"cyan\",\"orange\" ])\n","cmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n","_, ax = plt.subplots()\n","DecisionBoundaryDisplay.from_estimator(\n","        knn,\n","        X_train,\n","        cmap=cmap_light,\n","        ax=ax,\n","        response_method=\"predict\",\n","        plot_method=\"pcolormesh\",\n","        xlabel=wine.feature_names[0],\n","        ylabel=wine.feature_names[1],\n","        shading=\"auto\",\n",")\n","sns.scatterplot(\n","        x=X_train[:, 0],\n","        y=X_train[:, 1],\n","        hue=wine.target_names[y_train],\n","        palette=cmap_bold,\n","        alpha=1.0,\n","        edgecolor=\"black\",\n",")\n","plt.title(\"kNN Decision Boundary\")\n","plt.xlabel(wine.feature_names[0])\n","plt.ylabel(wine.feature_names[1])\n","plt.show()"],"metadata":{"id":"fFIg4I8NiCiC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(wine.data[:, 2:4], wine.target, test_size=0.2, random_state=0)\n","\n","# Create a KNN classifier with 3 nearest neighbours\n","knn = KNeighborsClassifier(n_neighbors=3)\n","\n","# Train the classifier on the training data\n","knn.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = knn.predict(X_test)\n","\n","# Evaluate the accuracy of the classifier\n","accuracy = knn.score(X_test, y_test)\n","print(f\"Accuracy: {accuracy:.3f}\")"],"metadata":{"id":"hfn7MTwQjsuV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot out decision boundary\n","cmap_light = ListedColormap([\"cornflowerblue\",\"cyan\",\"orange\" ])\n","cmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n","_, ax = plt.subplots()\n","DecisionBoundaryDisplay.from_estimator(\n","        knn,\n","        X_train,\n","        cmap=cmap_light,\n","        ax=ax,\n","        response_method=\"predict\",\n","        plot_method=\"pcolormesh\",\n","        xlabel=wine.feature_names[2],\n","        ylabel=wine.feature_names[3],\n","        shading=\"auto\",\n",")\n","sns.scatterplot(\n","        x=X_train[:, 0],\n","        y=X_train[:, 1],\n","        hue=wine.target_names[y_train],\n","        palette=cmap_bold,\n","        alpha=1.0,\n","        edgecolor=\"black\",\n",")\n","plt.title(\"kNN Decision Boundary\")\n","plt.xlabel(wine.feature_names[2])\n","plt.ylabel(wine.feature_names[3])\n","plt.show()"],"metadata":{"id":"n3-thzJKiFYm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(wine.data[:, [0,6]], wine.target, test_size=0.2, random_state=0)\n","\n","# Create a KNN classifier with 3 nearest neighbours\n","knn = KNeighborsClassifier(n_neighbors=3)\n","\n","# Train the classifier on the training data\n","knn.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = knn.predict(X_test)\n","\n","# Evaluate the accuracy of the classifier\n","accuracy = knn.score(X_test, y_test)\n","print(f\"Accuracy: {accuracy:.3f}\")"],"metadata":{"id":"68YcNPsmiMHn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot out decision boundary\n","cmap_light = ListedColormap([\"cornflowerblue\",\"cyan\",\"orange\" ])\n","cmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n","_, ax = plt.subplots()\n","DecisionBoundaryDisplay.from_estimator(\n","        knn,\n","        X_train,\n","        cmap=cmap_light,\n","        ax=ax,\n","        response_method=\"predict\",\n","        plot_method=\"pcolormesh\",\n","        xlabel=wine.feature_names[0],\n","        ylabel=wine.feature_names[6],\n","        shading=\"auto\",\n",")\n","sns.scatterplot(\n","        x=X_train[:, 0],\n","        y=X_train[:, 1],\n","        hue=wine.target_names[y_train],\n","        palette=cmap_bold,\n","        alpha=1.0,\n","        edgecolor=\"black\",\n",")\n","plt.title(\"kNN Decision Boundary\")\n","plt.xlabel(wine.feature_names[0])\n","plt.ylabel(wine.feature_names[6])\n","plt.show()"],"metadata":{"id":"s8WYW8RriMKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Xdo8C24CiMSr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X_fUDbihGcyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IeVZHzkrGc1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LkqnXZ7hGc_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CsQIqofOGdBy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Classification using Support Vector Machine\n","\n","A Support Vector Machine (SVM) is a type of algorithm used for sorting things into groups or predicting values. It's really good at sorting things when they can't be separated easily by a straight line. SVM finds the best line or boundary to separate different groups or values in the data. Like Logistic Regression before, SVM traditionally is used for binary classification, but can be extended to handle multi-class classification using specific techniques."],"metadata":{"id":"iINErftc8BDU"}},{"cell_type":"markdown","source":["Let's start by importing [the Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html), a well-known dataset often used for ML practice. Compared to previous datasets, the Iris dataset is much smaller with only 150 rows of data)"],"metadata":{"id":"dU7OJgHyIzZa"}},{"cell_type":"code","source":["# Import the libraries\n","from sklearn.datasets import load_iris\n","from sklearn import svm\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.inspection import DecisionBoundaryDisplay\n","%matplotlib inline\n","\n","# Load the iris dataset\n","iris = load_iris()"],"metadata":{"id":"zVmRysFIIDNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iris.data.shape"],"metadata":{"id":"B-Wx-1PJKk5i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This dataset only has four features:\n","*    Sepal Length\n","*    Sepal Width\n","*    Petal Length\n","*    Petal Width"],"metadata":{"id":"bOQdm08kJHAa"}},{"cell_type":"code","source":["iris.feature_names"],"metadata":{"id":"C60HNx--I_uF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll again split the dataset into training and testing arrays, and like before, we'll choose two features to train the model with. In this first example, we choose the first two features: alcohol and malic acid.\n","\n","One data is loaded, we can easily use the `SVC` function (SVC stands for C-Support Vector Classification, a specific implementation of SVM)."],"metadata":{"id":"b37TLUWeJgab"}},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(iris.data[:, :2], iris.target, test_size=0.2, random_state=None)\n","\n","# Create a SVM classifier\n","svm_model = svm.SVC()\n","\n","# Train the classifier on the training data\n","svm_model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = svm_model.predict(X_test)\n","\n","# Evaluate the accuracy of the classifier\n","accuracy = svm_model.score(X_test, y_test)\n","print(f\"Accuracy: {accuracy:.3f}\")"],"metadata":{"id":"4fYow9eBBz5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Start plotting\n","_, ax = plt.subplots()\n","x0, x1 = X_train[:, 0], X_train[:, 1]\n","disp = DecisionBoundaryDisplay.from_estimator(\n","      svm_model,\n","      X_train,\n","      response_method=\"predict\",\n","      cmap=plt.cm.coolwarm,\n","      alpha=0.8,\n","      ax=ax,\n","      xlabel=iris.feature_names[0],\n","      ylabel=iris.feature_names[1],\n",")\n","plt.scatter(x0, x1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n","plt.show()"],"metadata":{"id":"xtXiHh-KKbS7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What about training the model using a different pair of features?"],"metadata":{"id":"ujZwKAIKLdyC"}},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(iris.data[:, 2:], iris.target, test_size=0.2, random_state=None)\n","\n","# Create a SVM classifier\n","svm_model = svm.SVC()\n","\n","# Train the classifier on the training data\n","svm_model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = svm_model.predict(X_test)\n","\n","# Evaluate the accuracy of the classifier\n","accuracy = svm_model.score(X_test, y_test)\n","print(f\"Accuracy: {accuracy:.3f}\")"],"metadata":{"id":"3KPXqvjGhprC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Start plotting\n","_, ax = plt.subplots()\n","x0, x1 = X_train[:, 0], X_train[:, 1]\n","disp = DecisionBoundaryDisplay.from_estimator(\n","      svm_model,\n","      X_train,\n","      response_method=\"predict\",\n","      cmap=plt.cm.coolwarm,\n","      alpha=0.8,\n","      ax=ax,\n","      xlabel=iris.feature_names[2],\n","      ylabel=iris.feature_names[3],\n",")\n","plt.scatter(x0, x1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n","plt.show()"],"metadata":{"id":"CSw8nhlahptl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"s06MM8PmMeSU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"48TG8cwvMeU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"umvvhxo2MeXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tKofLUj2Meh-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ePJZ8Br7Mekg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Decision Trees\n","\n","Decision trees are a popular and intuitive machine learning algorithm used for both classification and regression tasks. They are called \"trees\" because they consist of a tree-like structure of nodes and branches. Each node represents a decision based on the value of a feature, and each branch represents the outcome of that decision. The decision tree algorithm determines the best feature and threshold to split the data at each node.\n","\n","Decision trees have several advantages, including their simplicity, interpretability, and ability to handle both numerical and categorical data. However, they can be prone to overfitting, especially when the trees are deep or when the dataset is noisy. Techniques such as pruning, limiting the tree depth, and using ensemble methods like Random Forests can help mitigate overfitting and improve the generalization performance of decision trees."],"metadata":{"id":"CYN1BlMngZR7"}},{"cell_type":"markdown","source":["Let's take a look at one using a subset of the [Titanic dataset](https://www.openml.org/search?type=data&sort=runs&id=40945&status=active). The features we'll look at are:\n","*    Pclass: The class of the ticket that was purchased\n","*    Age: The age of the passenger\n","*    SibSp: # of siblings / spouses on the Titanic\n","*    Parch: # of parents / children on the Titanic\n","*    Fare: The fare the passenger paid"],"metadata":{"id":"zPpPfLQ8v6Pv"}},{"cell_type":"code","source":["import pandas as pd\n","data = pd.read_csv(\n","    'https://raw.githubusercontent.com/iamamangosteen/aimlnotebooks/main/titanic.csv',\n","    usecols=['Survived', 'Pclass', 'Age', 'SibSp', 'Fare'])\n","data = data.dropna()\n","\n","data.head()"],"metadata":{"id":"0zScD_xaue97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Splitting data into training and testing data\n","from sklearn.model_selection import train_test_split\n","\n","y = data.pop('Survived')\n","X_train, X_test, y_train, y_test = train_test_split(data, y, random_state = 100)"],"metadata":{"id":"2y3DJ6GDwfaH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import tree\n","clf = tree.DecisionTreeClassifier()\n","clf = clf.fit(X_train, y_train)"],"metadata":{"id":"Gec6RX_XgZaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","# Make predictions using the testing dataset\n","predictions = clf.predict(X_test)\n","print(f\"Predictions: {predictions[:10]}\")\n","print(f\"     Actual: {np.array(y_test[:10])}\")"],"metadata":{"id":"930OnXlVt3Vs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the total accuracy:\n","from sklearn.metrics import accuracy_score\n","print(accuracy_score(y_test, predictions))"],"metadata":{"id":"Owp5mGjTxeY3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We've seen a lot of different parameters, so let's (finally) talk about how we can optimise them. One way is simple trial and error - you can train a bunch of models and observe the results and pick the best performing model. These options that we can customise are called **hyperparameters**, i.e. parameters that are not learned during the training process of a machine learning model but are set prior to training and control the learning process.\n","\n","However, sklearn helps automate this process with the `GridSearchCV` class. You can specify the hyperparameters you want to test and it will return the best combination:"],"metadata":{"id":"Cu5WU48g7xn9"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","# Creating a dictionary of parameters to use in GridSearchCV\n","params = {\n","    'criterion':  ['gini', 'entropy'],\n","    'max_depth':  [None, 2, 4, 6, 8, 10],\n","    'max_features': [None, 'sqrt', 'log2', 0.2, 0.4, 0.6, 0.8],\n","    'splitter': ['best', 'random']\n","}\n","\n","clf = GridSearchCV(\n","    estimator=tree.DecisionTreeClassifier(),\n","    param_grid=params,\n","    cv=5,\n","    n_jobs=5,\n","    verbose=1,\n",")\n","\n","clf.fit(X_train, y_train)\n","print(clf.best_params_)"],"metadata":{"id":"iWE209Nexebg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","data = pd.read_csv(\n","    'https://raw.githubusercontent.com/iamamangosteen/aimlnotebooks/main/titanic.csv',\n","    usecols=['Survived', 'Pclass', 'Age', 'SibSp', 'Fare'])\n","data = data.dropna()\n","\n","# Splitting data into training and testing data\n","from sklearn.model_selection import train_test_split\n","\n","y = data.pop('Survived')\n","X_train, X_test, y_train, y_test = train_test_split(data, y, random_state = 100)\n","\n","from sklearn import tree\n","clf = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth=4, max_features=0.8, splitter=\"best\")\n","clf = clf.fit(X_train, y_train)\n","\n","import numpy as np\n","# Make predictions using the testing dataset\n","predictions = clf.predict(X_test)\n","print(f\"Predictions: {predictions[:10]}\")\n","print(f\"     Actual: {np.array(y_test[:10])}\")\n","\n","# Check the total accuracy:\n","from sklearn.metrics import accuracy_score\n","print(accuracy_score(y_test, predictions))"],"metadata":{"id":"fmvDyH4i7w82"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise: Try out GridSearchCV\n","\n","Try to implement GridSearchCV on one of our previous datasets using a classifier of your choice:\n","*    Fashion MNIST\n","*    Digits MNIST\n","*    Iris\n","*    Titanic"],"metadata":{"id":"e_CCqTKHfG9a"}},{"cell_type":"code","source":[],"metadata":{"id":"8SSTroKG7xAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OPSgQ3837xCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"riM_s2lb1mZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"syekJRDHxeeQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Linear Regression with sklearn\n","\n","Here we'll use a new dataset: the [California housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset). This has over 20,000 rows of data, with 8 features.\n"],"metadata":{"id":"vp6XPXQ9DxKo"}},{"cell_type":"code","source":["#Import libraries\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Load California housing dataset\n","cal_houses = fetch_california_housing()"],"metadata":{"id":"Qt9UOJluTaqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cal_houses.data.shape"],"metadata":{"id":"mQ8bFGU0Tb9h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It's important to understand your datasets. In this dataset, the rows represent **block groups**. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). This is to keep individual responses to the Census private and anonymous.\n","\n","*    MedInc median income in block group\n","*    HouseAge median house age in block group\n","*    AveRooms average number of rooms per household\n","*    AveBedrms average number of bedrooms per household\n","*    Population block group population\n","*    AveOccup average number of household members\n","*    Latitude block group latitude\n","*    Longitude block group longitude\n","\n","A household is a group of people residing within a home. Since the average number of rooms and bedrooms in this dataset are provided per household, these columns may take surprisingly large values for block groups with few households and many empty houses, such as vacation resorts.\n","\n","The **target** variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000)."],"metadata":{"id":"0_ADZRG0WNf9"}},{"cell_type":"code","source":["cal_houses.feature_names"],"metadata":{"id":"r5GGHovwT7MV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cal_houses.target"],"metadata":{"id":"Smk5hN4GV6wM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(cal_houses.data[:, np.newaxis, 0], cal_houses.target, test_size=0.2, random_state=0)\n","\n","# Create a Linear Regression model\n","lin_reg_model = LinearRegression()\n","\n","# Train the Linear Regression model\n","lin_reg_model.fit(X_train, y_train)\n","\n","# Test Linear Regression model\n","y_pred = lin_reg_model.predict(X_test)\n","mse = mean_squared_error(y_test,y_pred)\n","print(f\"MSE: {mse:.3f}\")\n","\n","# Plot best fit line\n","plt.scatter(X_test, y_test, color=\"black\")\n","plt.plot(X_test, y_pred, color=\"blue\")\n","\n","plt.xlabel(cal_houses.feature_names[0])\n","plt.ylabel(\"Housing Price (aka target)\")\n"],"metadata":{"id":"ytGf1p3R8INJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that this reveals some additional information about the dataset: the price is capped at \\$500,000 dollars, so more expensive houses are also labeled as \\$500,000 houses. Should we remove these houses and rerun our training? Possibly! This is one of many judgement calls that data scientists need to make when analysing data."],"metadata":{"id":"D5Ba7iFnYK8K"}},{"cell_type":"code","source":[],"metadata":{"id":"lVDfpZAmUxzO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise: Calculating the MSE\n","\n","Mean squared error (MSE) measures the amount of error in statistical models. It assesses the average squared difference between the observed and predicted values. A model with no error would have a MSE of zero. Can you calculate the MSE for each feature? Which feature has the lowest MSE?"],"metadata":{"id":"wwGwPqAoYmCs"}},{"cell_type":"code","source":[],"metadata":{"id":"luRoFUupYu0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LEld7cgEY-O4"},"execution_count":null,"outputs":[]}]}